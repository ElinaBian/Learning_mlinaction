{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem -- Logistic Regression\n",
    "\n",
    "<font color = 'green'>What happens in logistic regression is we have a bunch of data, and with the data we try to build an equation to do classification for us.</font> The regression aspects means that we try to find a best-fit set of parameters.\n",
    "\n",
    "Finding the best fit is similar to regression, and in this method it’s how we train our classifier. We’ll use **optimization algorithms to find these best-fit parameters**. This best-fit stuff is where the name regression comes from. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification with logistic regression and the sigmoid function: a tractable step function\n",
    "\n",
    "We want an equation that could give all of features and predict the class. In the two-class case, the function will spit out a 0 or a 1, called Heaviside step function or sometimes just the step function. \n",
    "\n",
    "The problem with the Heaviside step function is that at the point where it steps from 0 to 1, it does so instantly. This instantaneous step is sometimes difficult to deal with. There’s another function that behaves in a similar fashion, but it’s much easier to deal with mathematically. This function is called the <font color = 'red'>**sigmoid**</font>. The sigmoid is given by the following equation:\n",
    "<font color = 'red'>$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$</font>\n",
    "\n",
    "图像是y轴上缓缓从0变成了1，x=0时，y=0.5。\n",
    "\n",
    "For the logistic regression classifier, we’ll <font color = 'green'>take our features and multiply each one by a weight and then add them up as z</font>. This result will be put into the sigmoid, and we’ll <font color = 'green'>get a number between 0 and 1 as $\\sigma(z)$</font>. Anything <font color = 'green'>above 0.5 we’ll classify as 1</font>, and anything <font color = 'green'>below 0.5 we’ll classify as a 0</font>. You can also think of logistic regression as a probability estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using optimization to find the best regression coefficients\n",
    "<br>\n",
    "<font size = 3.5>The input to the sigmoid function described will be z, where z is given by the following:\n",
    "$$z = w_0x_0 + w_1x_1 +w_2x_2 +...+ w_nx_n$$\n",
    "In vector notation we can write this as <font color = 'red'>$z=w^Tx$</font>.\n",
    "<br><br>\n",
    "The vector x is our input data, and we want to find the best coefficients w, so that this classifier will be as successful as possible. In order to do that, we need to consider some ideas from optimization theory.\n",
    "</font>\n",
    "\n",
    "### 2.1 Gradient ascent 梯度上升\n",
    "\n",
    "The first optimization algorithm we’re going to look at is called gradient ascent. Gradient ascent is based on the idea that if we want to find the maximum point on a function, then **the best way to move is in the direction of the gradient**. \n",
    "<br><br>\n",
    "We write the gradient with the symbol $\\bigtriangledown$ and the gradient of a function $f(x,y)$ is given by the equation\n",
    "<br><br>\n",
    "$$\\bigtriangledown f(x,y) = \\left(\\begin{array}{c} \\frac{\\partial f(x,y)}{\\partial x} \\\\ \\frac{\\partial f(x,y)}{\\partial y} \\end{array}\\right)$$\n",
    "<br><br>\n",
    "The gradient ascent algorithm takes a step in the direction given by the gradient. The gradient operator will always point in the direction of the greatest increase. We’ve talked about direction, but I didn’t mention anything to do with **magnitude of movement**(移动量). The magnitude, or step size, we’ll take is given by the parameter $\\alpha$. In vector notation we can write the **gradient ascent algorithm** as\n",
    "<br><br>\n",
    "<font color = 'red'>$$w := w + \\alpha \\bigtriangledown_w f(w)$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train: using gradient ascent to find the best parameters\n",
    "\n",
    "There are 100 data points and each point has two numeric features: <font color = 'blue'>$X_1$ and $X_2$</font>. We’ll try to use gradient ascent to fit the best parameters for the logistic regression model to our data. In addition, this sets the value of <font color = 'blue'>$X_0$</font> to 1.0, which is a convention we use.\n",
    "\n",
    "1.Start with the weights all set to 1.\n",
    "\n",
    "2.Repeat R number of times: \n",
    "\n",
    "(1)Calculate the gradient of the entire dataset; \n",
    "\n",
    "(2)Update the weights vector by alpha*gradient;\n",
    "\n",
    "(3)Return the weights vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset():\n",
    "    data_mat = []\n",
    "    label_mat = []\n",
    "    fr = open('/Users/jbian/Desktop/CU-life/summerlearning/mlinaction/dataset/testSet_chap5.txt')\n",
    "    for line in fr.readlines():\n",
    "        line_arr = line.strip().split()\n",
    "        data_mat.append([1.0, float(line_arr[0]), float(line_arr[1])])  #X0 was set to 1.0\n",
    "        label_mat.append(int(line_arr[2]))\n",
    "    return data_mat, label_mat\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def grad_ascent(data_mat, class_labels):\n",
    "    data_matrix = np.mat(data_mat)                 #convert to NumPy matrix\n",
    "    label_mat = np.mat(class_labels).transpose()   #convert to NumPy matrix\n",
    "    m,n = np.shape(data_matrix)\n",
    "    alpha = 0.001\n",
    "    max_cycles = 500\n",
    "    weights = np.ones((n,1))\n",
    "    for k in range(max_cycles):                 #heavy on matrix operations\n",
    "        h = sigmoid(data_matrix*weights)        #matrix multiplication\n",
    "                                                #如果之前没有变成matrix，则*代表逐个元素相乘，dot才代表矩阵乘\n",
    "        error = (label_mat - h)                 #vector subtraction\n",
    "        weights = weights + alpha * data_matrix.transpose()* error      #matrix multiplication\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='/Users/jbian/Desktop/CU-life/summerlearning/mlinaction/dataset/testSet_chap5.txt' mode='r' encoding='UTF-8'>\n",
      "-0.017612\t14.053064\t0\n",
      "\n",
      "-1.395634\t4.662541\t1\n",
      "\n",
      "-0.752157\t6.538620\t0\n",
      "\n",
      "-1.322371\t7.152853\t0\n",
      "\n",
      "0.423363\t11.054677\t0\n",
      "\n",
      "0.406704\t7.067335\t1\n",
      "\n",
      "0.667394\t12.741452\t0\n",
      "\n",
      "-2.460150\t6.866805\t1\n",
      "\n",
      "0.569411\t9.548755\t0\n",
      "\n",
      "-0.026632\t10.427743\t0\n",
      "\n",
      "0.850433\t6.920334\t1\n",
      "\n",
      "1.347183\t13.175500\t0\n",
      "\n",
      "1.176813\t3.167020\t1\n",
      "\n",
      "-1.781871\t9.097953\t0\n",
      "\n",
      "-0.566606\t5.749003\t1\n",
      "\n",
      "0.931635\t1.589505\t1\n",
      "\n",
      "-0.024205\t6.151823\t1\n",
      "\n",
      "-0.036453\t2.690988\t1\n",
      "\n",
      "-0.196949\t0.444165\t1\n",
      "\n",
      "1.014459\t5.754399\t1\n",
      "\n",
      "1.985298\t3.230619\t1\n",
      "\n",
      "-1.693453\t-0.557540\t1\n",
      "\n",
      "-0.576525\t11.778922\t0\n",
      "\n",
      "-0.346811\t-1.678730\t1\n",
      "\n",
      "-2.124484\t2.672471\t1\n",
      "\n",
      "1.217916\t9.597015\t0\n",
      "\n",
      "-0.733928\t9.098687\t0\n",
      "\n",
      "-3.642001\t-1.618087\t1\n",
      "\n",
      "0.315985\t3.523953\t1\n",
      "\n",
      "1.416614\t9.619232\t0\n",
      "\n",
      "-0.386323\t3.989286\t1\n",
      "\n",
      "0.556921\t8.294984\t1\n",
      "\n",
      "1.224863\t11.587360\t0\n",
      "\n",
      "-1.347803\t-2.406051\t1\n",
      "\n",
      "1.196604\t4.951851\t1\n",
      "\n",
      "0.275221\t9.543647\t0\n",
      "\n",
      "0.470575\t9.332488\t0\n",
      "\n",
      "-1.889567\t9.542662\t0\n",
      "\n",
      "-1.527893\t12.150579\t0\n",
      "\n",
      "-1.185247\t11.309318\t0\n",
      "\n",
      "-0.445678\t3.297303\t1\n",
      "\n",
      "1.042222\t6.105155\t1\n",
      "\n",
      "-0.618787\t10.320986\t0\n",
      "\n",
      "1.152083\t0.548467\t1\n",
      "\n",
      "0.828534\t2.676045\t1\n",
      "\n",
      "-1.237728\t10.549033\t0\n",
      "\n",
      "-0.683565\t-2.166125\t1\n",
      "\n",
      "0.229456\t5.921938\t1\n",
      "\n",
      "-0.959885\t11.555336\t0\n",
      "\n",
      "0.492911\t10.993324\t0\n",
      "\n",
      "0.184992\t8.721488\t0\n",
      "\n",
      "-0.355715\t10.325976\t0\n",
      "\n",
      "-0.397822\t8.058397\t0\n",
      "\n",
      "0.824839\t13.730343\t0\n",
      "\n",
      "1.507278\t5.027866\t1\n",
      "\n",
      "0.099671\t6.835839\t1\n",
      "\n",
      "-0.344008\t10.717485\t0\n",
      "\n",
      "1.785928\t7.718645\t1\n",
      "\n",
      "-0.918801\t11.560217\t0\n",
      "\n",
      "-0.364009\t4.747300\t1\n",
      "\n",
      "-0.841722\t4.119083\t1\n",
      "\n",
      "0.490426\t1.960539\t1\n",
      "\n",
      "-0.007194\t9.075792\t0\n",
      "\n",
      "0.356107\t12.447863\t0\n",
      "\n",
      "0.342578\t12.281162\t0\n",
      "\n",
      "-0.810823\t-1.466018\t1\n",
      "\n",
      "2.530777\t6.476801\t1\n",
      "\n",
      "1.296683\t11.607559\t0\n",
      "\n",
      "0.475487\t12.040035\t0\n",
      "\n",
      "-0.783277\t11.009725\t0\n",
      "\n",
      "0.074798\t11.023650\t0\n",
      "\n",
      "-1.337472\t0.468339\t1\n",
      "\n",
      "-0.102781\t13.763651\t0\n",
      "\n",
      "-0.147324\t2.874846\t1\n",
      "\n",
      "0.518389\t9.887035\t0\n",
      "\n",
      "1.015399\t7.571882\t0\n",
      "\n",
      "-1.658086\t-0.027255\t1\n",
      "\n",
      "1.319944\t2.171228\t1\n",
      "\n",
      "2.056216\t5.019981\t1\n",
      "\n",
      "-0.851633\t4.375691\t1\n",
      "\n",
      "-1.510047\t6.061992\t0\n",
      "\n",
      "-1.076637\t-3.181888\t1\n",
      "\n",
      "1.821096\t10.283990\t0\n",
      "\n",
      "3.010150\t8.401766\t1\n",
      "\n",
      "-1.099458\t1.688274\t1\n",
      "\n",
      "-0.834872\t-1.733869\t1\n",
      "\n",
      "-0.846637\t3.849075\t1\n",
      "\n",
      "1.400102\t12.628781\t0\n",
      "\n",
      "1.752842\t5.468166\t1\n",
      "\n",
      "0.078557\t0.059736\t1\n",
      "\n",
      "0.089392\t-0.715300\t1\n",
      "\n",
      "1.825662\t12.693808\t0\n",
      "\n",
      "0.197445\t9.744638\t0\n",
      "\n",
      "0.126117\t0.922311\t1\n",
      "\n",
      "-0.679797\t1.220530\t1\n",
      "\n",
      "0.677983\t2.556666\t1\n",
      "\n",
      "0.761349\t10.693862\t0\n",
      "\n",
      "-2.168791\t0.143632\t1\n",
      "\n",
      "1.388610\t9.341997\t0\n",
      "\n",
      "0.317029\t14.739025\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 理解一下数据格式\n",
    "\n",
    "fr = open('/Users/jbian/Desktop/CU-life/summerlearning/mlinaction/dataset/testSet_chap5.txt')\n",
    "print(fr)\n",
    "\n",
    "for line in fr.readlines():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, -0.017612, 14.053064], [1.0, -1.395634, 4.662541], [1.0, -0.752157, 6.53862], [1.0, -1.322371, 7.152853], [1.0, 0.423363, 11.054677], [1.0, 0.406704, 7.067335], [1.0, 0.667394, 12.741452], [1.0, -2.46015, 6.866805], [1.0, 0.569411, 9.548755], [1.0, -0.026632, 10.427743]]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "[[ 1.       -0.017612 14.053064]\n",
      " [ 1.       -1.395634  4.662541]\n",
      " [ 1.       -0.752157  6.53862 ]\n",
      " [ 1.       -1.322371  7.152853]\n",
      " [ 1.        0.423363 11.054677]\n",
      " [ 1.        0.406704  7.067335]\n",
      " [ 1.        0.667394 12.741452]\n",
      " [ 1.       -2.46015   6.866805]\n",
      " [ 1.        0.569411  9.548755]\n",
      " [ 1.       -0.026632 10.427743]]\n",
      "(100, 3)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data_arr, label_mat = load_dataset()\n",
    "print(data_arr[:10])\n",
    "print(label_mat[:10])\n",
    "\n",
    "\n",
    "# 理解一下Numpy matrix\n",
    "data_matrix = np.mat(data_arr)\n",
    "print(data_matrix[:10])\n",
    "print(np.shape(data_matrix))\n",
    "label_mat = np.mat(label_mat).transpose() \n",
    "print(np.shape(label_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.12414349]\n",
      " [ 0.48007329]\n",
      " [-0.6168482 ]]\n"
     ]
    }
   ],
   "source": [
    "# calculate weights\n",
    "data_arr, label_mat = load_dataset()\n",
    "grad_ascent(data_arr, label_mat)\n",
    "weights = grad_ascent(data_arr, label_mat)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyze: plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_fit(weights):\n",
    "    import matplotlib.pyplot as plt\n",
    "    data_mat, label_mat = load_dataset()\n",
    "    weights = np.asarray(weights)         # return self as an ndarray object; equivalent to np.asarray(self).\n",
    "    data_arr = np.array(data_mat)\n",
    "    n = np.shape(data_arr)[0] \n",
    "    x_cord1 = []\n",
    "    y_cord1 = []\n",
    "    x_cord2 = []\n",
    "    y_cord2 = []\n",
    "    for i in range(n):\n",
    "        if int(label_mat[i])== 1:    # class 1--red, mark--s; class 2--green\n",
    "            x_cord1.append(data_arr[i,1])\n",
    "            y_cord1.append(data_arr[i,2])\n",
    "        else:\n",
    "            x_cord2.append(data_arr[i,1])\n",
    "            y_cord2.append(data_arr[i,2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_cord1, y_cord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(x_cord2, y_cord2, s=30, c='green')\n",
    "    x = np.arange(-3.0, 3.0, 0.1)\n",
    "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X1'); plt.ylabel('X2')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_best_fit(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+00, -9.00000000e-01, -8.00000000e-01, -7.00000000e-01,\n",
       "       -6.00000000e-01, -5.00000000e-01, -4.00000000e-01, -3.00000000e-01,\n",
       "       -2.00000000e-01, -1.00000000e-01, -2.22044605e-16,  1.00000000e-01,\n",
       "        2.00000000e-01,  3.00000000e-01,  4.00000000e-01,  5.00000000e-01,\n",
       "        6.00000000e-01,  7.00000000e-01,  8.00000000e-01,  9.00000000e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 关于np.arange\n",
    "\n",
    "x = np.arange(-1, 1, 0.1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train: Stochastic Gradient Ascent 随机梯度上升\n",
    "The previous optimization algorithm, gradient ascent, uses the whole dataset on each update. This was fine with 100 examples, but with billions of data points containing thousands of features. An alternative to this method is to update the weights using **only one instance at a time**. (只用一个样本点更新回归系数)\n",
    "\n",
    "Stochastic gradient ascent is an example of an online learning algorithm. This is known as **online**(在线学习) because we can incrementally update the classifier as new data comes in rather than all at once. The all-at-once method is known as <font color = 'red'>**batch processing**(批处理)</font>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoc_grad_ascent(data_mat, class_labels):\n",
    "    m,n = np.shape(data_mat)\n",
    "    alpha = 0.01\n",
    "    weights = np.ones(n)   #initialize to all ones\n",
    "    for i in range(m):\n",
    "        h = sigmoid(sum(data_mat[i]*weights))\n",
    "        error = class_labels[i] - h\n",
    "        weights = weights + alpha * error * np.asarray(data_mat[i])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.01702007  0.85914348 -0.36579921]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPXV+PHPCQlhC2FJ2EJiEEIQERBiUCkoLoDUve5rtRWx7lXp9uvTPu3T11ODuKIC7lbcumh9rLKpCCiyKYgKgbAGwhaQEAhkPb8/MmAIWWaSmbn3zpz365UXmTt3Zs5kLvfM/Z7vIqqKMcYY468YpwMwxhjjLZY4jDHGBMQShzHGmIBY4jDGGBMQSxzGGGMCYonDGGNMQCxxGGOMCYglDmOMMQGxxGGMMSYgsU4HEApJSUmanp7udBjGGOMZy5cvL1TVZH/2jcjEkZ6ezrJly5wOwxhjPENENvu7rzVVGWOMCYglDmOMMQGxxGGMMSYgljiMMcYEJOSJQ0ReFJFdIvJNjW1/FJFtIrLC9zOunseOFZFcEckTkV+HOlZjjDGNC8cVx8vA2Dq2P6aqg30/H9S+U0RaAE8DFwD9gWtFpH9IIzXGGNOokCcOVZ0P7G3CQ7OBPFXdoKplwJvAJUENzhhjTMCcrHHcJSJf+5qyOtZxfwqQX+P2Vt+2OonIeBFZJiLLdu/eHexYjfGc/KJ87v7gbrKfy+buD+4mvyi/8QcZ4wenEsezQG9gMLAdmFzHPlLHtnoXSFfV6aqapapZycl+DX40JmLlF+UzaOogpi2fxtKCpUxbPo1BUwdZ8jBB4UjiUNWdqlqpqlXAc1Q3S9W2FUitcbsnUBCO+IzxupzPcjhQdoDyqnIAyqvKOVB2gJzPchyOzEQCRxKHiHSvcfMy4Js6dlsKZIhILxFpCVwDvBeO+IzxusXbFh9NGkeUV5WzZNsShyIykSQc3XHfABYBmSKyVUR+BuSIyCoR+RoYBdzv27eHiHwAoKoVwF3ALGA18LaqfhvqeI2JBMNShhEXE3fMtriYOLJT6rq4NyYwolpv2cCzsrKy1CY5NNHsSI3jSHNVXEwc7Vq2Y+WElaQmpjb+BCbqiMhyVc3yZ18bOW5MBEpNTGXlhJXcPvR2sntkc/vQ2y1pmKCJyGnVjYkU+UX55HyWw+JtixmWMoyJwyf6ffJPTUzlqXFPhThCE40scRjjUrWbm1bsWMGMVTPsysE4zpqqjHEp61Jr3MquOIxxkZpNU5v3bXZtl9rmNKEZ77PEYYxL1G6aiqmjQcANXWqtCc1YU5UxLlG7aaqKKgDEN/vOkS61E4dPdCxGsCY0Y1ccxrhGXaO9AZLbJpOemE52SrYrmoRsVLqxxGGMSwxLGcaKHSuOOSnHxcRxVf+rXNWttr44nW5CM+FjTVXGuMTE4RNp17Ld0alC3NI0VZtX4jShY4nDGJfwymjvcMRpa4m4m81VZYxxFZtnyxk2V5UxxrOs15b7WeIwxriK9dpyP0scxhhXsbVE3M8ShzGmTk4VqEPZa8uK7sER8uK4iLwIXAjsUtUBvm2TgIuAMmA9cIuq7qvjsZuAYqASqPC3cGPFcWOax+kC9ZG5sBZsWUCVVhEjMYxIG9GsAZBOvye3c1tx/GVgbK1tc4ABqjoQWAv8poHHj1LVwf6+IWNM8zldoE5NTGXi8IlsKdrCmsI1rNy5kmnLpzFo6qAmXyU4/Z4iScgTh6rOB/bW2jbbt6Y4wBdAz1DHYUy0akrzjBsK1ME+0bvhPUUKN9Q4bgU+rOc+BWaLyHIRGR/GmIyJCIu3LibjqQymLJ3C0oKlTF021a9v7W4oUAf7RO+G9xQpHE0cIvI7oAKYUc8uw1V1CHABcKeIjGzgucaLyDIRWbZ79+4QRGuMt+QX5XPWy2dRWll6dFuFVlBcWtzot3Y3TCsS7BO9G95TpHAscYjIzVQXza/Xeir0qlrg+3cX8A5Q7xGjqtNVNUtVs5KTk0MRsgkx6/ESXDmf5RyTNI6o0IpGv7W7YfqTYJ/o3fCeIkVYphwRkXTg/Rq9qsYCjwJnqWqdlwci0haIUdVi3+9zgD+p6szGXs96VXmP9XgJvuznsllasLTO+24eeDMJ8QmuX8HvSO+qJduWuGZa+UgVSK+qkE+rLiJvAGcDSSKyFfgD1b2o4oE5IgLwhapOEJEewPOqOg7oCrzjuz8WeN2fpGG8qaFCqJumFPeSYSnD+GrHV1RUVRyzvWVMS97NfZeS8hLXr+CXmphqn78LhaNX1bWq2l1V41S1p6q+oKp9VDXV1812sKpO8O1b4EsaqOoGVR3k+zlZVf8S6liNc7zW48ULzWoTh08koWUCsTE/fD+MbxHPxZkXH00aYN1STeDc0KvKGE/1eDnSrDZt+TSWFixt9viCUDnSpj9h6ASye2Rz12l3se7udWwu2uypJG3cx1YANK4wcfhEZqyacVyNw409XrzUrFZXU4+bVvA7UsM4Umu5YeANvPb1a66vvUQ7W4/DuEa4CqH5Rfn8/uPf82HehyBwQZ8L+POoP/v9WvUVnbN7ZLP4tsXBDjfo3NIRoXYcsRJLpVbSIqYFFVUV1kEizFxVHDfGX+EohOYX5XPKs6dQVFp0dNsrK1/h3TXvsuqOVX6doNz0jb0pjjRhOd1bqfaVW4VvMokjxXw3X8lFO6txmIjgb7E657Mc9pfuP267P4PijoiEgWRHkvTi2xbz1LinHPlGX1eHiNqs9uJOdsVhPK92k0dD3UsXb1uMcnzzbBVVfp+g3PKN3evqunKrzUtXctHErjiM5wUyGd6wlGEIctz2GGICOkG54Ru719W+couVWAQ52n3Yi1dy0cISh/G8QMaATBw+kfbx7Y/bnhCfcPQE5eYxGs2NzU3vrfYUIBOyJrDoZ4uOdh+2KUHcy3pVGc+7+4O7mbZ82nHF6tuH3l5nUbWhXlVu6XFUl+bGVlcvphYxLeiX1K/ZiyQZ73PbQk7GhFSgxerUxFRevuxldj60k50P7uTlS18+esJ082I/zY2trl5MpZWlQVkkyUQXSxzG84I562kopj4JVvNQc2NrqBeTmxKkcT/rVWUiQrDGgAR7jEYgPb5CHVtjvZgaS0K1R3lb01b0sisO42nBLvYGe4xGMJu+mhtb7cfX1lAS8sr8XCY8LHEYzwrFySzYi/00p3mpdlIEmhVbzfc2qOsg4lvE+9311c21HxN+1lRlPCtUkw021OwVaHNN/+T+LC9YThVVR7f507zUUBNXsN5bIHODeW3aexNaljiMZ4X7ZBZovSK/KJ9317x7TNIAaBPXptHmpXDMwBtIXcjr83MdYXWa4LCmKuNZ4V7DI9DmmpzPcigpLzlmmyBcmnlpoycrt33Dj4T5uaxOEzxhSRwi8qKI7BKRb2ps6yQic0Rkne/fjvU89mbfPutE5OZwxGu8Idwns0BP5nXtryirC1c3+lpuW9gq2LUfJ1idJnjCdcXxMjC21rZfAx+pagbwke/2MUSkE9VrlA8DsoE/1JdgTPQJ98ks0JN5c07+bvyG7/X5udx2FedlYUkcqjof2Ftr8yXAK77fXwEureOhY4A5qrpXVb8H5nB8AjJRLJwns0BP5s05+YcrKbpp7qpQc9tVnJeFba4qEUkH3lfVAb7b+1S1Q437v1fVjrUe8yDQSlX/x3f798AhVX2kodeyuapMqAS6SmE4VjVsasHXzfNyhUK0vd9ABTJXldsTx0NAfK3EUaKqk+t4/vHAeIC0tLShmzdvDtVbMcY1mnMyDHRyyEgQruWJvcgrkxzuFJHuAL5/d9Wxz1ag5qfaEyio68lUdbqqZqlqVnJyctCDNQ2LpiYPN6mv4Pvj13/c6GdRX5v/29+9HbbPz5/jJpjHltfrNG7h5BXHJGCPqv5VRH4NdFLVibUe0wlYDgzxbfoSGKqqteslx7CmqvCyJgDnZD+XzdKCpfXe39BnUdcVB1R3Ge7QqkPIPz9/jhs7tuqnqogcvyhZU7nuikNE3gAWAZkislVEfgb8FThfRNYB5/tuIyJZIvI8gC9B/BlY6vv5U2NJw4SfdXN0Tl0F35oa+iyOFO9jap0GFA3L5+fPcWPH1vFWbS3ixhcW8/7X2x2LISwjx1X12nruOreOfZcBP69x+0XgxRCFZoLAujk6Z+LwicxYNeOYk2ttdX0WR9r6UxNTOVRxiMMVhxt9TLD5c9zM3zLfji2fDbsPMHnOWv7z9XY6tonj0sEpjsViU46YZouU6Si86Ei33SMF30MVh1i9ezUVWnF0n9qfRe3mn9pXHHU9JhQaO27yi/LJLcw97nGxEhtVx9aOosM88dE63l6WT3xsDPec04efjzyR9q3qv9IMNVs61jRbfe3QH17/Ia99/Zpn5wVyw7xGgcbgT02godqGomGrIzQW690f3M3U5VOpqKo45nHxLeJZd/c6Tx1LTbGvpIxn563n5c83UaXK9cNO4M5RfUhOiA/J67myO244WeIIv9rdHG8YeAMXzLjAs0VNNxRlmxpDY11O6yuod2nbhfTE9LB2U20o1vriHNR1ECsmrAh5bE4pKavgpc82MfXT9RworeCyU1O4/7y+pHZqE9LXDSRxWFOVCYraM63e/cHdIZ/dNZTCMTttqGJobNbb+pqIrup/Vdg/m4ZirS/OEWkjwhVeWJVVVPHW0i088VEehQdKOe+krjw0JpPMbglOh3Ycmx3XhITXC+ZuiD9UMbhxHqy6eCXO5qqqUt79ahvnPfopv//3t5yY1JZ/3nEGz9+c5cqkAZY4TIh4fV4gN8TvbwyBDpDzyky3XomzqVSVj9fsZNyTC7jvrRW0jY/lpVtO463bT2foCZ2cDq9BVuMwIeGGGkFzuCF+GyAXuZZu2kvOzDUs3fQ9aZ3a8MDovlw0sAcxMcEb0BcoK45b4nAFr88L5Ib4G4shGueb8rI1O/YzaWYuH63ZRXJCPPecm8HVWam0jHW+8ccShyUOEyXq63mU3SObxbctdiAiU5cte0p4bO5a3l2xjXbxsdxxdm9+emY6bVoGoX9S+/ZQXHz89oQE2L/f76exXlXGRImgDL4M0onHHG93cSlTPl7H60u2ECPC+JEncsdZvenQpmXwXqSuz66h7UFgicMYD6s95UiTeh45cOKJdPsPlzP90w28sHAjZZVVXH1aKveck0G3xFZOhxYUljiM8bDaU454sZYUSQ6XV/Lqok08M289+0rKuXBgdx4YnUmvpLZOhxZUljiMCYFwTlfS2IA/E3oVlVX8Y/lWHp+7jh37D3NW32QeGpPJgJREp0MLCUscxgRZ7S6yK3asYMaqGdZFNgKpKh9+s4NHZuWyofAgp6Z14LGrB3NG785OhxZSljiMCTI3TFdiQktVWZhXSM7MXFZtKyKjSzum3TiU0f27BnVxJb8kJNTfuSFELHEYE2RumK4kIA6ceMIqyL3GVuTvI2fmGj5fv4eUDq155MpBXHZqCi2cGrznQM83SxzGBJnn1ieJ9C63Qeo1lrermEdmrWXmtzvo3LYl/3Vhf64/PY342BZBCNJbHBuuKCKZIrKixs9+Ebmv1j5ni0hRjX3+y6l4jfFXtEzO15hA59Byq237DvHQ31cy+rH5LMwr5P7z+vLpxFHc+qNeUZk0wCUjx0WkBbANGKaqm2tsPxt4UFUvDOT5bOS4cZobpitxkqvm0Gqo5tDA+W/vwTKe/iSPvy3aDAI3nX4CvxjVh05tgzh4z0W8OHL8XGB9zaRhjJdFexdZL3cQOFBawQsLNvLcgg2UlFVwxdCe3HteX1I6tHY6NNdwS+K4BnijnvvOEJGVQAHVVx/fhi8sY0xTeK6DAFBaUcnri7cw5eM89hwsY+zJ3XhwTF/6dImQTgJB5HjiEJGWwMXAb+q4+0vgBFU9ICLjgHeBjHqeZzwwHiAtLS1E0RqnuWEdcNM4V3UQaKTXWGWV8s5X23hszlq27TvEGSd25lcX9GNwaocwB+odjtc4ROQS4E5VHe3HvpuALFUtbGg/q3FEJle1m3tNmCcy9MJnparM+W4nk2blsm7XAU5JSWTi2Ex+1Ccp/GMxXMBrNY5rqaeZSkS6ATtVVUUkm+peYHvCGZxxDy+3mzsuzBMZun0OrUXr95Azaw1fbdlHr6S2PH3dEC4Y0M3RhZS8xNHEISJtgPOB22tsmwCgqlOBK4A7RKQCOARco05fInlJhE2X7cV286Bzy2fqRxxu7CDwzbYicmblMn/tbrq1b8VfLz+FK4b2JLaF8wspeYmjiUNVS4DOtbZNrfH7FGBKuOOKGBE2Xbar2s2d4pbP1C1x+Glj4UEmz87l/a+3k9g6jt+O68dNZ6TTKi46x2E0lxuaqozxS1DWnjBRZef+wzzx0TreWppPyxYx3DmqN+NH9iaxdZzToXmaJQ7jGeFsN7feW95WVFLOs5+u5+XPN1JZpVw/LI27zulDl4TIWEjJaY73qgoF61Xl08QRs9HO1T2CmvqZBrs24tJj61BZJS99vpGp89ZTXFrBJYN68MvzM0nr3MaxmI5yS32qHl7rVWWMq0Rk7y0XnJhCqbyyireW5vPkR+vYVVzKuf268OCYTE7q3t7p0H7gsbpQQyxxRLJIny47RFzde8stn6lL4qiqUv7v6wIenbOWzXtKOC29I89cP4Ss9E5hjSPaWOKIZBH+LTNUXNF7q6FmDTc0Mzp8bKkq89buJmdmLqu376dftwRe/GkWozK7ROXgvXCzxGFcwy0FaVf03oqEZo0Qtekv37yXh2fmsmTjXtI6teGJawZz0cAeNngvjKw4blzBbQVpx6dFd2nxOSBBfg+5O4qZNCuXuat3ktQunnvO7cM1p6XRMtYjg/dc/placdx4jtsK0m4c9dwgl/fYaY78vSU8Nnct73y1jXbxsTw0JpNbhqfTpqXHTl8uqQsFg8f+8iZSubog7QWR0LRVy+7iUp7+JI8ZizcTI8L4kSdyx1m96dDGowspeTyB12SJw7iCKwrSgYjgb/hB0b7p3WD3Hy7n+fkbeH7hRkorqrgqK5V7z82gW6IN3nMLSxzGFVxRkA5EqL/he71Zowl/h8Pllfxt0WaemZfH9yXlXDiwO788vy8nJrcLQYCmOSxxGFdw+zTcYRfJVy21kl9FZRX//HIrj89dx/aiw4zISGLimH6c0jPRoQBNYxpMHCLSHkhW1fW1tg9U1a9DGpmJOp4rSJum8SVFVWXmNzuYNDuXDbsPMji1A5OvGsSZvZOC8zrWnBgy9SYOEbkKeBzYJSJxwE9Vdanv7peBIaEPzxjjF481bX2WV8jDM9fw9dYi+nRpx7QbhzK6f9fgDt6LwA4DbtHQFcdvgaGqut23+t7fROS3qvovwEbaGOMmHvkGvbJbBpOeX8zCvEJSOrRm0hUDuXxIT1rY4D1PaShxxKrqdgBVXSIio4D3RaQn4PxoFWOc5LFv+GFX6++T16knk0feyIeZw+m0fT+/v7A/1w9Ls4WUPKqhxLFfRHofqW/4rjzOBt4FTg5WACKyCSgGKoGK2iMXpfra9QlgHFBCdZPZl8F6fWOaxCPf8B3j+/sU7DvEE3PX8ffl+bSOa8E9I07kthG9SGjVxIWUwlG3sNpIoxpKHL+iVpOUqhaLyFjgN0GOY5SqFtZz3wVAhu9nGPCs719jjEt9f7CMZ+bl8cqizaDw0zN7ceeo3nRuF1+9Q1NPzuGoW1htpFENJY5XgGkiMllVKwBEpCswGcgE/hSG+AAuAV7V6km1vhCRDiLS/UgzmjHGPQ6WVvDiwo1Mn7+Bg2UVXHZqT+4/P4OeHWstpBSOk7M1J4ZMQ4ljKPC/wFcici9wCvBLIAe4KYgxKDBbRBSYpqrTa92fAuTXuL3Vt+2YxCEi44HxAGlpaUEMz5gmiqImj7KKKt5YsoWnPl5H4YEyRvfvyoNjMunb1cGTdIT9jd2k3sShqt8DE3xJYy5QAJyuqluDHMNwVS0QkS7AHBFZo6rza9xfV3eL44rzvoQzHapnxw1yjMYpXj75RkGTR2WV8u8V23h0zlq2fn+I00/sxPSb+jEkraPToZkQamgcRwfgYarrCWOpLk5/KCL3qurHwQpAVQt8/+4SkXeAbKBm4tgK1Bw+3JPqJGaiQRScfL1IVflo9S4mzcold2cxJ/doz18uO4WRGUm2kFIUaKip6kvgGeBOX41jtogMBp4Rkc2qem1zX1xE2gIxvqJ7W2A0x9dO3gPuEpE3qU5iRVbfMMY5izfsIWdWLss3f0+vpLZMue5Uxg3oHp6FlMJRt7DaSKMaShwjazdLqeoK4EwRuS1Ir98VeMf3DSUWeF1VZ4rIBN/rTQU+oPpqJ4/q7ri3BOm1janm5eawMPq2oIhJs3KZl7ubru3j+ctlA7gqK5W4Fk1YSKmpJ+dwfB72mTfKVgA07haOVdNC9RouX/HNX5v3HGTy7LW8t7KAxNZx/OLs3tx8ZroN3oswtgKgMW7g8SaPXfsP8+TH63hzST6xLYQ7R/Vm/MjeJLZu4uA9EzEscRh38/LJ16NNHkWHypn26Xpe/GwjFZXKtdlp3H1OH7q0t4WUTDVLHMbdPHry9WLd5FBZJS9/vompn65n/+FyLhnUg/vP78sJnds6HZpxGUscxoSCh7oRl1dW8fayfJ6Yu45dxaWMykzmoTH96N+j6cu/mshmicOYKFVVpby/ajuPzs5l054Ssk7oyJTrhpDdq5PToRmXs8RhTJRRVT5du5ucmbl8t30//bol8MLNWZzTr4sN3jN+scRhTBRZvvl7cmauYfHGvaR2as3jVw/mokE9bCElExBLHMZEolrF+bVJaUwacSNz+p5BUruW/PfFJ3NtdhotY5sweM9EPUscxoSC092Ifa+9tX0yj/3oev414BzalR3igfl/49ZPXqNtvP3XN01nR48xwTrJu6gLbmGbRJ4+4ypmDB4HKLcteYc7vvgHHQ8XQ/ybYY3FRB5LHMYE66Tugi64xYfLeX7BRp4f/xyH4uK5ctVc7v3sDXoU17fApgNclGCPcmNMLmaJw5gIcLi8khmLt/D0J3nsPVjGuI1f8ssFr9Fnb7CXzwkCFyRYv1/bheNu3MAShzEeVlFZxb++2sbjc9ZSUHSYERlJPDQmk4GpFzodmolgljiMcVoTmklUlVnf7uSR2bnk7TrAoJ6JTLpyEMP7JP3wWK/O8WVczxKHMU4LsJnk8/WFPDwzl5X5++id3JapNwxhzMndjh28Z+3yJoQscZi6WbEwcKH4ll8jGazq2pucc3/GgtSB9EhsRc5PBnL5kBRim7KQUnPYsRH1HEscIpIKvAp0A6qA6ar6RK19zgb+DWz0bfqXqtZeWtaEghULAxeik+b6Tik8OuIG/tNvBB1Livh/Pz6JG04/wbmFlJp7bLixGc2NMbmYk1ccFcADqvqliCQAy0Vkjqp+V2u/BapqlT4TdXa068wTw6/l7YHnE19Rxj2fvc5tS94h4cmS0L94Q1cVzeXGqxI3xuRijiUOVd0ObPf9Xiwiq4EUoHbiMCaqfN8qgamnX8HLQy6kKiaGG7/8D3cteoukkqLwBWFXnKYBrqhxiEg6cCqwuI67zxCRlUAB8KCqfhvG0IwJPV8zycG4VryUdTHThv2EAy1bc/k3H3PfwtdJ3b/L6QiNOYbjiUNE2gH/BO5T1drXi18CJ6jqAREZB7wLZNTzPOOB8QBpaWkhjNiY4Crbu483l27hyY/yKDxQyvn9u/LgA1eQWbjZ6dCMqZOoqnMvLhIHvA/MUtVH/dh/E5Clqg3On5CVlaXLli0LTpDRynrOhFxVlfLeygImz8klf+8hhvXqxMSx/Rh6Qkfn//4NrcvRUCG5ubE5/b6jmIgsV9Usf/Z1sleVAC8Aq+tLGiLSDdipqioi2UAMsCeMYUYv+08aMqrKx2t2MWlWLmt2FNO/e3teufUURmYk/TAWw81//1DGZrUVT3CyqWo4cCOwSkRW+Lb9FkgDUNWpwBXAHSJSARwCrlEnL5GMaaalm/by8IdrWLb5e9I7t+Gpa0/lx6d0JybcCyk19s3euqeaBjjZq2oh0OD/FlWdAkwJT0TGhM7q7fuZNCuXj9fsIjkhnv+5dABXn5ZKXLgH7x3R2Dd7N1/xGMc5Xhw3JpJt2VPCo3Ny+ffKAhLiY5k4NpOfnplOm5b2X894lx29JvK4oMC6q/gwT32UxxtLthDbQphwVm8mjOxNYpu4sLy+MaFkicNEHgcLrEWHypk+fz0vLtxEeWUV12Sncs85GXRp3yrkr+2oYCVrq614giUOY4LgcHklr3y+iWfmrafoUDkXDerBA+f3JT2prdOhhUewkrXVVjzBEocxzVBRWcXfl2/libnr2LH/MGdnJvPg6EwGpCQ6HVrD7Ju9aQZLHMY0pJ4mmKqE9ny4cA2TZ+eyofAgQ9I68Pg1gzn9xM4OBNkE9s3eNIMlDmMaUitpKLAg/VQmjbyJVa9/SWbXBJ67KYvzTupy7EJKxkQwSxwm8oSoGear7n3JOetmFp0wiJ77djD5ykFcemoKLcI9eM8Yh1niMJEnyM0w6zqn8siIG5mVeSadD+7jj3Omcu3KmcRPLQ/q6/jNBd2N63xtq5lEDUscxtRj275DPH7BvfxzwDm0KS/llwte49Zl/6Zd2SFnA3PjfE5WM4kqljiMqWXPgVKembeevy3aDP3P4tZl7/GLL/5Op0N2cjQGLHEYc9SB0gqeX7CB5xdspKSsgiuHpnLvPRfTY3sd62JYE4yJYpY4TNQrrahkxhdbmPJJHnsPlnHBgG48MLovfbokwBWbnA4vsrixPmMCZonDeEMITjiVVcq/vtzK43PXsW3fIYb36cxDY/oxOLVDM4P1Q7SeQN1YnzEBs8RhvCGIJxxVZfZ3O3lkVi7rdh1gYM9EHv7JQH6UkdTMIAPQnPdjPZiMwyxxmKiyaP0eHp65hhX5+zgxqS3PXD+ECwZ089bgvUi+IjGeYInDHCtCm1C+2VZEzqxc5q/dTbf2rXj4J6fwkyE9iXVqISVjPMzRxCEiY4EngBbA86r611r3xwOvAkOpXmv8alXdFO44o0qEtUFvLDzI5Nm5vP/1djq0ieN3407ixjNOoFVcC6dDC1yEJnXjPY7OLh+bAAAOMUlEQVQlDhFpATwNnA9sBZaKyHuq+l2N3X4GfK+qfUTkGuBh4OrwR2uCJkwnvx1Fh3nio3W8vSyfli1iuPucPtw28kTat/LwQkqRkNStPhMRnLziyAbyVHUDgIi8CVwC1EwclwB/9P3+D2CKiIiqajgDNUHU1JOfnyecfSVlPPvpel7+bBNVqlw/LI27z8kgOSG+iQGHSLSeQO3KKCI4mThSgPwat7cCw+rbR1UrRKQI6AwUhiVC4x6NnHAOlVXy0ucbmTpvPcWlFVw2OIX7z+9Laqc2YQowQHYCNR7mZOKoqxtL7SsJf/ap3lFkPDAeIC0trXmRGc8or6zizaX5PPnROnYXl3LeSV14cEwm/bq1dzo0YyKWk4ljK5Ba43ZPoKCefbaKSCyQCOyt68lUdTowHSArK8uasprKI00oVVXK/31dwKNz1rJ5TwmnpXfk2euHkJXeyenQjIl4TiaOpUCGiPQCtgHXANfV2uc94GZgEXAF8LHVN0LM5U0oqsq83N3kzMpl9fb99OuWwEs/PY2zM5O9NRajKTyS1E3kcyxx+GoWdwGzqO6O+6KqfisifwKWqep7wAvA30Qkj+orjWucitc0U329qY7w4+S3bNNecmbmsmTTXtI6teGJawZz0cAexETLQkouT+omekgkfoHPysrSZcuWOR2Gqamhq4FGjsE1O/bzyKxc5q7eRXJCPPecm8HVWam0jI2ywXs2jsOEkIgsV9Usf/a1kePGtbbsKeGxuWt5d8U22sXH8tCYTG4Znk6bllF62EbCOA4TEaL0f6Bxs93FpUz5eB2vL9lCjAi3j+zNhLNOpEOblk6HZozBEodxkf2Hy3lu/gZeWLiR0ooqrspK5d5zM+iW2Mrp0IwxNVjiMI47XF7Jq4s28cy89ewrKefCgd15YHQmvZLaOh2aMaYOljhMeNTRlbRCYvjHaRfy+KR57Nh/mJF9k5k4JpMBKYkOBWmM8YclDhMeNXr9qCoffrODR2bnsmH3QU7t0IrHrh7MGb07OxigB9g4DuMSljhMWC1cV8jDM9ewalsRGV3aMf3GoZzfv2vkD94LButya1zCEocJi5X5+8iZtYbP8vaQ0qE1j1w5iMtOTaGFWwbv2RgJY/xmicOEVN6uYh6ZtZaZ3+6gc9uW/OGi/lw3LI34WJctpGRjJIzxmyUOsG+bIVCw7xCPz13LP5ZvpU3LWO47L4OfjziRdvFRdsjZsWUiUJT9L66HfdsMmr0Hy3jmkzxe/WIzKNwyvBe/OLs3ndu5bCGlmtqHcAp2O7ZMBLLEYYLiYGkFLyzcyPT5Gygpq+DyIT25//y+pHRo7XRojbOTuDEBscRhmqW0opI3Fm9hyid5FB4oY8zJXXlwdCYZXa2LqDGRyhKHaZLKKuXfK7bx6Jy1bP3+EGec2Jnnbsrk1LSOTocWXDZGwpjjWOIwAVFV5q7exaRZa1i78wADUtrzv5efwo/6JEXuWIy63pcVt00Us8QBNiLXT4s37OHhmWv4css+eiW1Zcp1pzJuQPfIXkipucVtO7ZMBLLEAfbNsRHfFhQxaVYu83J307V9PP97+SlcMbQncS0iZCGlhk7uzS2c27FlIpAjiUNEJgEXAWXAeuAWVd1Xx36bgGKgEqjwd3UqExybCg8yec5a/m9lAYmt4/jNBf24+cx0WsU5MHgvlOMhGnp8pDa/GdMMTl1xzAF+41t3/GHgN8Cv6tl3lKoWhi80s3P/YZ78aB1vLc0nrkUMd47qzfiRvUlsHedcUDYewhjXcCRxqOrsGje/AK5wIg5zrKKScqbOX89Ln22kolK5blgad53Thy4JtpCSMeYHbqhx3Aq8Vc99CswWEQWmqer08IUVPQ6VVfLy55t4dl4exaUVXDo4hfvP60ta5zZOh+Y8K24bc5yQJQ4RmQt0q+Ou36nqv337/A6oAGbU8zTDVbVARLoAc0RkjarOr+f1xgPjAdLS0podfzQor6ziraX5PPnROnYVl3Juvy48OCaTk7qHcAoOr7HitjHHCVniUNXzGrpfRG4GLgTOVVWt5zkKfP/uEpF3gGygzsThuxqZDpCVlVXn85lqVVXK+6u28+jsXDbtKeG09I48ff0QTkvv5HRoxhgPcKpX1Viqi+FnqWpJPfu0BWJUtdj3+2jgT2EMM+KoKp+u3U3OzFy+276fft0SeOmnp3F2ZrL7B+9Zk5ExruFUjWMKEE918xPAF6o6QUR6AM+r6jigK/CO7/5Y4HVVnelQvJ63fPNeHp6Zy5KNe0nr1IYnrhnMRQN7eGfwnjUZGeMaTvWq6lPP9gJgnO/3DcCgcMYViXJ3FDNpVi5zV+8kqV08f7rkZK45LY2WsREyeK8+tg6GMSHjhl5VJgTy95bw2Ny1vPPVNtq1jOWhMZncMjydNi2j5CO3cR/GhEyUnEWix+7iUp7+JI8ZizcTI8JtI07kjrN607FtS6dDM8ZECEscEaL4cDnPzd/A8ws3UlpRxVVZPbnn3Ay6J3pgISVjjKdY4vC4w+WVvPbFZp7+JI/vS8oZd0o3HhidSe/kdk6HZoyJUJY4PKqisop/fbmNx+eupaDoMCMyknhoTCYDe3ZwOjRjTISzxOExqsqsb3cwaVYu63cfZFBqBx65chBn9klyOjR3sXEfxoSMJQ4P+TyvkIdn5bIyfx99urRj6g1DGHNyN/cP3nOCdbk1JmQscXjA11v3MWlWLgvWFdIjsRU5Vwzk8lNTiI2UhZSMMZ5iicPF1u8+wOTZuXywagcd28Tx/358EjecfoIzCykZY4yPJQ4X2l50iCfmruPvy7fSKjaGe8/N4OcjepHQysGFlIwxxscSh4t8f7CMZ+bl8cqizaBw0xkncOeoPiS1i3c6NGOMOcoShwscLK3gxYUbmT5/AwfLKrh8SE/uOy+Dnh1tISVjjPtY4nDY53mF3PPmVxQeKGN0/648OCaTvl2ty6gxxr0scTisV3Jb+vdI5L7zMhiS1tHpcIwxplGWOBzWPbE1r96a7XQYxhjjNxsIYIwxJiCWOIwxxgTEkcQhIn8UkW0issL3M66e/caKSK6I5InIr8MdpzHGmOM5WeN4TFUfqe9OEWkBPA2cD2wFlorIe6r6XbgCNMYYczw3N1VlA3mqukFVy4A3gUscjskYY6Kek4njLhH5WkReFJG6+qGmAPk1bm/1bauTiIwXkWUismz37t3BjtUYY4xPyBKHiMwVkW/q+LkEeBboDQwGtgOT63qKOrZpfa+nqtNVNUtVs5KTk4PyHowxxhwvZDUOVT3Pn/1E5Dng/Tru2gqk1rjdEygIQmjGGGOawZHiuIh0V9XtvpuXAd/UsdtSIENEegHbgGuA6/x5/uXLlxeKyOagBBseSUCh00E0gcUdPl6MGSzucGpuzCf4u6NTvapyRGQw1U1Pm4DbAUSkB/C8qo5T1QoRuQuYBbQAXlTVb/15clX1VFuViCxT1Syn4wiUxR0+XowZLO5wCmfMjiQOVb2xnu0FwLgatz8APghXXMYYYxrn5u64xhhjXMgShztMdzqAJrK4w8eLMYPFHU5hi1lU6+3haowxxhzHrjiMMcYExBKHS4jIn30j6VeIyGxfDzPXE5FJIrLGF/s7ItLB6ZgaIyJXisi3IlIlIq7vOePFyT59M0LsEpG6utq7koikisgnIrLad3zc63RM/hCRViKyRERW+uL+75C/pjVVuYOItFfV/b7f7wH6q+oEh8NqlIiMBj72dZ9+GEBVf+VwWA0SkZOAKmAa8KCqLnM4pHr5JvtcS43JPoFr3T7Zp4iMBA4Ar6rqAKfj8YeIdAe6q+qXIpIALAcu9cDfWoC2qnpAROKAhcC9qvpFqF7Trjhc4kjS8GlLA9OruImqzlbVCt/NL6ge4e9qqrpaVXOdjsNPnpzsU1XnA3udjiMQqrpdVb/0/V4MrKaB+fHcQqsd8N2M8/2E9PxhicNFROQvIpIPXA/8l9PxNMGtwIdOBxFhAprs0wSHiKQDpwKLnY3EPyLSQkRWALuAOaoa0rgtcYRRIxM/oqq/U9VUYAZwl7PR/qCxuH37/A6ooDp2x/kTs0cENNmnaT4RaQf8E7ivVkuAa6lqpaoOpvqKP1tEQto86ORCTlHH34kfgdeB/wB/CGE4fmssbhG5GbgQOFddUjQL4G/tdjbZZxj5agT/BGao6r+cjidQqrpPROYBY6l7DsCgsCsOlxCRjBo3LwbWOBVLIERkLPAr4GJVLXE6ngh0dLJPEWlJ9WSf7zkcU0TyFZlfAFar6qNOx+MvEUk+0ptRRFoD5xHi84f1qnIJEfknkEl1b5/NwARV3eZsVI0TkTwgHtjj2/SF23uDichlwFNAMrAPWKGqY5yNqn4iMg54nB8m+/yLwyE1SkTeAM6mesbWncAfVPUFR4NqhIj8CFgArKL6/yHAb31z5rmWiAwEXqH6+IgB3lbVP4X0NS1xGGOMCYQ1VRljjAmIJQ5jjDEBscRhjDEmIJY4jDHGBMQShzHGmIBY4jAmBHwzrW4UkU6+2x19t08QkZkisk9E3nc6TmOawhKHMSGgqvnAs8BffZv+CkxX1c3AJOBGp2IzprkscRgTOo8Bp4vIfcCPgMkAqvoRUOxkYMY0h81VZUyIqGq5iDwEzARG+6ZFN8bz7IrDmNC6ANgOeGIxI2P8YYnDmBARkcFUr9x3OnC/b4U5YzzPEocxIeCbafVZqtd02EJ1QfwRZ6MyJjgscRgTGrcBW1R1ju/2M0A/ETlLRBYAfwfOFZGtIuLamXmNqYvNjmuMMSYgdsVhjDEmIJY4jDHGBMQShzHGmIBY4jDGGBMQSxzGGGMCYonDGGNMQCxxGGOMCYglDmOMMQH5/9s9mn9jubgvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_arr, label_mat = load_dataset()\n",
    "weights = stoc_grad_ascent(data_arr, label_mat)\n",
    "print(weights)\n",
    "plot_best_fit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the result of classification is not as good as before. However, directly comparing the stochastic gradient ascent algorithm to the gradient ascent is unfair, because the gradient ascent code had 500 iterations over the entire dataset.\n",
    "\n",
    "Now, we do some changes to present SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoc_grad_ascent_iter(data_mat, class_labels, num_iter=150):\n",
    "    m,n = np.shape(data_mat)\n",
    "    weights = np.ones(n)   #initialize to all ones\n",
    "    for j in range(num_iter):\n",
    "        data_index = list(range(m))\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i)+0.0001                                #alpha decreases with iteration, does not \n",
    "            rand_index = int(np.random.uniform(0,len(data_index)))    #go to 0 because of the constant\n",
    "            h = sigmoid(sum(data_mat[rand_index]*weights))\n",
    "            error = class_labels[rand_index] - h\n",
    "            weights = np.asarray(weights)\n",
    "            weights = weights + alpha * error * np.asarray(data_mat[rand_index])\n",
    "            del(data_index[rand_index])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that alpha changes on each iteration. Alpha decreases as the number of iterations increases, but it never reaches 0 because there’s a constant term so that after a large number of cycles, new data still has some impact. \n",
    "\n",
    "Alpha function is that it decreases by 1/(j+i). Notice that j is the index of the number of times you go through the dataset, and i is the index of the example in the training set. This gives an alpha that isn’t strictly decreasing when j<<max(i). \n",
    "\n",
    "The second improvement is that we randomly select each instance to use in updating the weights. This will reduce the periodic variations(周期性波动). The way you randomly select a value from a list of integers and then delete it from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.75567215  0.84235181 -2.07455889]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_arr, label_mat = load_dataset()\n",
    "weights = stoc_grad_ascent_iter(data_arr, label_mat)\n",
    "print(weights)\n",
    "plot_best_fit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: estimating horse fatalities from colic 从疝气病症预测病马的死亡率\n",
    "### 3.1 Prepare: dealing with missing values in the data\n",
    "\n",
    "--Use the feature’s mean value from all the available data.\n",
    "\n",
    "--Fill in the unknown with a special value like -1.\n",
    "\n",
    "--Ignore the instance.\n",
    "\n",
    "--Use a mean value from similar items.\n",
    "\n",
    "--Use another machine learning algorithm to predict the value.\n",
    "\n",
    "Here, we replace missing values with 0, because 0 makes weights = weights via the update process and we delete data if missing label appears. And the data after preprocessing saved as horseColicTest.txt and horseColicTraining.txt.\n",
    "\n",
    "### 3.2 Test: classifying with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: \n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0\n",
    "\n",
    "def colic_test():\n",
    "    fr_train = open('/Users/jbian/Desktop/CU-life/summerlearning/mlinaction/dataset/horse/horseColicTraining.txt')\n",
    "    fr_test = open('/Users/jbian/Desktop/CU-life/summerlearning/mlinaction/dataset/horse/horseColicTest.txt')\n",
    "    training_set = []\n",
    "    training_labels = []\n",
    "    for line in fr_train.readlines():\n",
    "        curr_line = line.strip().split('\\t')\n",
    "        line_arr =[]\n",
    "        for i in range(21):            # 100个样本 20个特征 第21列为label\n",
    "            line_arr.append(float(curr_line[i]))\n",
    "        training_set.append(line_arr)\n",
    "        training_labels.append(float(curr_line[21]))\n",
    "    train_weights = stoc_grad_ascent_iter(np.array(training_set), training_labels, 1000)\n",
    "    error_count = 0\n",
    "    num_test_vec = 0.0\n",
    "    for line in fr_test.readlines():\n",
    "        num_test_vec += 1.0\n",
    "        curr_line = line.strip().split('\\t')\n",
    "        line_arr =[]\n",
    "        for i in range(21):\n",
    "            line_arr.append(float(curr_line[i]))\n",
    "        if int(classify_vector(np.array(line_arr), train_weights))!= int(curr_line[21]):\n",
    "            error_count += 1\n",
    "    error_rate = (float(error_count)/num_test_vec)\n",
    "    print('the error rate of this test is: %f' % error_rate)\n",
    "    return error_rate\n",
    "\n",
    "def multi_test(num_tests = 10):\n",
    "    error_sum = 0.0\n",
    "    for k in range(num_tests):\n",
    "        error_sum += colic_test()\n",
    "    print('after %d iterations the average error rate is: %f' % (num_tests, error_sum/float(num_tests)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate of this test is: 0.313433\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.313433\n",
      "the error rate of this test is: 0.432836\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.402985\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.402985\n",
      "the error rate of this test is: 0.388060\n",
      "the error rate of this test is: 0.417910\n",
      "the error rate of this test is: 0.388060\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.417910\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.373134\n",
      "the error rate of this test is: 0.268657\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.268657\n",
      "the error rate of this test is: 0.238806\n",
      "the error rate of this test is: 0.388060\n",
      "the error rate of this test is: 0.373134\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.358209\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.328358\n",
      "the error rate of this test is: 0.417910\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.298507\n",
      "the error rate of this test is: 0.283582\n",
      "the error rate of this test is: 0.373134\n",
      "the error rate of this test is: 0.388060\n",
      "the error rate of this test is: 0.313433\n",
      "the error rate of this test is: 0.283582\n",
      "the error rate of this test is: 0.283582\n",
      "the error rate of this test is: 0.402985\n",
      "the error rate of this test is: 0.447761\n",
      "the error rate of this test is: 0.238806\n",
      "the error rate of this test is: 0.358209\n",
      "the error rate of this test is: 0.447761\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.417910\n",
      "the error rate of this test is: 0.373134\n",
      "the error rate of this test is: 0.313433\n",
      "the error rate of this test is: 0.358209\n",
      "the error rate of this test is: 0.462687\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.343284\n",
      "the error rate of this test is: 0.373134\n",
      "the error rate of this test is: 0.447761\n",
      "after 50 iterations the average error rate is: 0.351343\n"
     ]
    }
   ],
   "source": [
    "multi_test(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above are not so bad, because we have around 30% missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
