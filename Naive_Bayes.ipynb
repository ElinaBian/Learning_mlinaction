{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem -- Naive Bayes\n",
    "<br>\n",
    "**Pros**: Works with a small amount of data, handles multiple classes \n",
    "\n",
    "**Cons**: Sensitive to how the input data is prepared\n",
    "\n",
    "**Works with**: Nominal values\n",
    "\n",
    "## 1. Introduction\n",
    "<font size = 3.5>\n",
    "<br>\n",
    "<font color = 'green'>**Naive**</font>: 朴素，即为最简单最原始的假设条件下\n",
    "<br><br>\n",
    "**Put simply, we choose the class with the higher probability. That’s Bayesian decision theory in a nutshell: choosing the decision with the highest probability.**\n",
    "<br><br>\n",
    "For examplt, we have an equation for the probability of a piece of data belonging to Class 1 (the circles): p1(x, y), and we have an equation for the class belonging to Class 2 (the triangles): p2(x, y). To classify a new measurement with features (x, y), we use the following rules:\n",
    "<br><br>\n",
    "If $p_1(x, y) > p_2(x, y)$, then the class is 1. \n",
    "<br><br>\n",
    "If $p_2(x, y) > p_1(x, y)$, then the class is 2.\n",
    "<br><br>\n",
    "For the kNN method, the classification problem above is too calculatedly costing. If we use decision trees from chapter 2, and make a split of the data once along the x-axis and once along the y-axis, the result might not satisfied. Given this problem, the best choice would be the probability comparison we just discussed.\n",
    "<br><br>\n",
    "A useful way to manipulate conditional probabilities is known as **Bayes’ rule**.\n",
    "<br><br>\n",
    "$$p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}$$\n",
    "<br><br>\n",
    "忍不住想补充一点：\n",
    "    先验概率即$p(\\theta)$是基于experience or other inference，而后验概率的意义则是已知果反推因为何。\n",
    "    <br><br>\n",
    "    课上学的：prior probability is what we believe about $\\theta$, and posterior probability is what we would adjust our belief about $\\theta$ given some data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document classification with naive Bayes\n",
    "<br>\n",
    "<font size = 3.5>The first assumption of naive Bayes is the independence. By independence, I mean statistical independence; one feature or word is just as likely by itself as it is next to other words. The other assumption we make is that every feature is equally important.\n",
    "<br>\n",
    "<br>\n",
    "Make a quick filter for an online message board that flags a message as inappropriate if the author uses negative or abusive language. Filtering out this sort of thing is common because abusive postings make people not come back and can hurt an online community. We’ll have two categories: abusive and not. We’ll use 1 to represent abusive and 0 to represent not abusive.\n",
    "</font>\n",
    "\n",
    "### 2.1 Prepare: making word vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    posting_list=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0,1,0,1,0,1]            #1 is abusive, 0 not\n",
    "    return posting_list, class_vec \n",
    "\n",
    "def create_vocab_list(dataset):          #extract the unique word list from the text\n",
    "    vocabulary_set = set([])                              #create empty set\n",
    "    for document in dataset:\n",
    "        vocabulary_set = vocabulary_set | set(document)   #union of the two sets\n",
    "    return list(vocabulary_set)\n",
    "\n",
    "\n",
    "# whether every unique word existing in every posting_list\n",
    "def wordset_to_vec(vocab_list, input_set):           #output is a vector containing 0 or 1\n",
    "    return_vec = [0]*len(vocab_list)                 #create a vector containing only 0\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return return_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes classifier training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb0(train_matrix, train_category):\n",
    "    # train_matrix comes from return vec and train category comes from class_vec\n",
    "    \n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    p_abusive = sum(train_category)/float(num_train_docs)\n",
    "    p0_numer = np.zeros(num_words) \n",
    "    p1_numer = np.zeros(num_words)              \n",
    "    p0_denom = 0.0                      \n",
    "    p1_denom = 0.0\n",
    "    # 此处分子是一个元素个数等于vocab长度的numpy数组\n",
    "    # 而分母是总词数\n",
    "    # p0 or p1的0和1表示class是否为abusive\n",
    "    \n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_numer += train_matrix[i]        # 提取有abusive词汇的vector，是一个vector\n",
    "            p1_denom += sum(train_matrix[i])   # 计算该vector对应的posting_list的总词数，是一个number   \n",
    "        else:\n",
    "            p0_numer += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "    p1_vec = p1_numer/p1_denom                 # class为abusive的posting里各个词出现的概率\n",
    "    p0_vec = p0_numer/p0_denom          \n",
    "    return p0_vec, p1_vec, p_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "['maybe', 'food', 'dog', 'worthless', 'love', 'him', 'stop', 'posting', 'dalmation', 'licks', 'quit', 'take', 'to', 'so', 'I', 'is', 'flea', 'mr', 'my', 'park', 'please', 'not', 'steak', 'has', 'cute', 'garbage', 'how', 'help', 'ate', 'buying', 'stupid', 'problems']\n"
     ]
    }
   ],
   "source": [
    "# create dataset for simple test\n",
    "\n",
    "import numpy as np\n",
    "posting_list, class_list = load_dataset()\n",
    "print(posting_list)\n",
    "print(class_list)\n",
    "vocab_list = create_vocab_list(posting_list)\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use wordset_to_vec function to transfer the text into training set\n",
    "# append every vector\n",
    "\n",
    "train_mat = []\n",
    "for posting in posting_list:\n",
    "    train_mat.append(wordset_to_vec(vocab_list, posting))\n",
    "train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.04166667 0.         0.04166667 0.08333333\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.         0.\n",
      " 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667\n",
      " 0.125      0.         0.04166667 0.         0.04166667 0.04166667\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.04166667 0.\n",
      " 0.         0.04166667]\n",
      "[0.05263158 0.05263158 0.10526316 0.10526316 0.         0.05263158\n",
      " 0.05263158 0.05263158 0.         0.         0.05263158 0.05263158\n",
      " 0.05263158 0.         0.         0.         0.         0.\n",
      " 0.         0.05263158 0.         0.05263158 0.         0.\n",
      " 0.         0.05263158 0.         0.         0.         0.05263158\n",
      " 0.15789474 0.        ]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p0_vec, p1_vec, p_abusive = train_nb0(train_mat, class_list)\n",
    "print(p0_vec)\n",
    "print(p1_vec)\n",
    "print(p_abusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the 10th number in p1_vec is the largest probabilities, and the 10th word in vocab_list is *stupid*, which means that the word *stupid* is most indicative of a class 1 (abusive).\n",
    "### 2.4 modifying the classifier for real-world conditions\n",
    "### 2.4.1 Problem about probability 0\n",
    "When we attempt to classify a document, we multiply a lot of probabilities together to get the probability that a document belongs to a given class. This will look something like $P(w_0|1)P(w_1|1)P(w_2|1)$. If any of these numbers are 0, then when we multiply them together we get 0. \n",
    "\n",
    "To lessen the impact of this, we’ll initialize all of our occur- rence counts to 1, and we’ll initialize the denominators to 2. \n",
    "\n",
    "The <font color = 'blue'>train_nb0 function</font> changes as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb0(train_matrix, train_category):\n",
    "    # train_matrix comes from return vec and train category comes from class_vec\n",
    "    \n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    p_abusive = sum(train_category)/float(num_train_docs)\n",
    "    p0_numer = np.ones(num_words)          #changes here\n",
    "    p1_numer = np.ones(num_words)          #changes here    \n",
    "    p0_denom = 2.0                         #changes here\n",
    "    p1_denom = 2.0                         #changes here\n",
    "    \n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_numer += train_matrix[i]        \n",
    "            p1_denom += sum(train_matrix[i])      \n",
    "        else:\n",
    "            p0_numer += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "    p1_vec = p1_numer/p1_denom                 \n",
    "    p0_vec = p0_numer/p0_denom          \n",
    "    return p0_vec, p1_vec, p_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Problem underflow\n",
    "We might do too many multiplications of small numbers. When we go to calculate the product $P(w_0|c_i)P(w_1|c_i)P(w_2|c_i)...P(w_N|c_i)$ and many of these numbers are very small, we’ll get underflow, or an incorrect answer.\n",
    "\n",
    "One solution to this is to take the natural logarithm of this product.\n",
    "\n",
    "The <font color = 'blue'>train_nb0 function</font> changes as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "def train_nb0(train_matrix, train_category):\n",
    "    # train_matrix comes from return vec and train category comes from class_vec\n",
    "    \n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    p_abusive = sum(train_category)/float(num_train_docs)\n",
    "    p0_numer = np.ones(num_words) \n",
    "    p1_numer = np.ones(num_words)              \n",
    "    p0_denom = 2.0                      \n",
    "    p1_denom = 2.0\n",
    "    \n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_numer += train_matrix[i]        \n",
    "            p1_denom += sum(train_matrix[i])      \n",
    "        else:\n",
    "            p0_numer += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "    p1_vec = [log(x) for x in p1_numer/p1_denom]              #changes here           \n",
    "    p0_vec = [log(x) for x in p0_numer/p0_denom]              #changes here\n",
    "    return p0_vec, p1_vec, p_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Naive Bayes classify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_nb(vec_to_classify, p0_vec, p1_vec, p_class1):\n",
    "    p1 = sum(vec_to_classify * p1_vec) + log(p_class1)     #element-wise multiplication\n",
    "                                                           #to multiply the first elements of both vectors, and so on\n",
    "    p0 = sum(vec_to_classify * p0_vec) + log(1.0 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function to wrap up everything properly and save you some time from typing all the code\n",
    "# 便利函数，封装所有函数（包括load dataset, create vocabulary list, wordset to vector, train_nb, classify_nb）\n",
    "\n",
    "def testing_nb():\n",
    "    list_of_posts, list_classes = load_dataset()\n",
    "    vocab_list = create_vocab_list(list_of_posts)\n",
    "    train_mat=[]\n",
    "    for post_in_doc in list_of_posts:\n",
    "        train_mat.append(wordset_to_vec(vocab_list, post_in_doc))\n",
    "    p0_vector, p1_vector, p_abusive = train_nb0(np.array(train_mat), np.array(list_classes))\n",
    "    test_entry = ['love', 'my', 'dalmation']\n",
    "    this_doc = np.array(wordset_to_vec(vocab_list, test_entry))\n",
    "    print(test_entry,'classified as: ', classify_nb(this_doc, p0_vector, p1_vector, p_abusive))\n",
    "    test_entry = ['stupid', 'garbage']\n",
    "    this_doc = np.array(wordset_to_vec(vocab_list, test_entry))\n",
    "    print(test_entry,'classified as: ', classify_nb(this_doc, p0_vector, p1_vector, p_abusive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testing_nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Prepare: the bag-of-words document model\n",
    "Up until this point we’ve treated the presence or absence of a word as a feature. This could be described as a **set-of-words model**. \n",
    "\n",
    "If a word appears <font color = 'red'>more than once in a document</font>, that might convey some sort of information about the document over just the word occurring in the document or not. This approach is known as a **bag-of-words model**. A bag of words can have multiple occurrences of each word, whereas a set of words can have only one occurrence of each word.“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从0到1变成num + 1\n",
    "\n",
    "def wordbag_to_vec(vocab_list, input_set):\n",
    "    return_vec = [0]*len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] += 1\n",
    "    return return_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: classifying spam email with naive Bayes\n",
    "### 3.1 Prepare: tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " \"I've\",\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to split the text\n",
    "\n",
    "my_text = 'This book is the best book for Python or M.L. I\\'ve ever laid eyes upon.'\n",
    "my_text.split()\n",
    "# That works well, but the punctuation is considered part of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'for', 'Python', 'or', 'M', 'L', 'I', 've', 'ever', 'laid', 'eyes', 'upon', '']\n",
      "['This', 'book', 'is', 'the', 'best', 'book', 'for', 'Python', 'or', 'M', 'L', 'I', 've', 'ever', 'laid', 'eyes', 'upon', '']\n",
      "['', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '.', '. ', \"'\", ' ', ' ', ' ', ' ', '.']\n",
      "['This', 'book', 'is', 'the', 'best', 'book', 'for', 'Python', 'or', 'M.L.', \"I've\", 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "# We could use regular expressions to split up the sentence on anything that isn’t a word or number.\n",
    "import re\n",
    "reg_ex = re.compile('\\W+')      # \\W Matches any character which is not a word character\n",
    "                                # *：匹配零次或多次；+：匹配一次或多次；？：匹配零次或一次\n",
    "list_of_token = reg_ex.split(my_text)\n",
    "print(list_of_token)\n",
    "print(re.split('\\W+', my_text))\n",
    "print(re.split('\\w+', my_text))\n",
    "print(re.split('\\s+', my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 've',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in list_of_token if len(tok)>0]               # 去掉空字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 've',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in list_of_token if len(tok)>0]       # 去掉空字符并统一转换为小写字母"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test: cross validation with naive Baye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(big_string):    # input is big string and output is word list\n",
    "    import re\n",
    "    list_of_tokens = re.split('\\W+', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if len(tok) > 2] \n",
    "# 太短的word也没有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def spam_test():\n",
    "    doc_list=[]\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    for i in range(1,26):\n",
    "        word_list = text_parse(open('/Users/elinabian 1/Desktop/CU-life/summerlearning/mlinaction/dataset/email/spam/%d.txt' % i, encoding = \"ISO-8859-1\").read())\n",
    "        doc_list.append(word_list)     #every text record as a list \n",
    "        full_text.extend(word_list)    #every text record combining into one list\n",
    "        class_list.append(1)           #class 1 represents spam\n",
    "        word_list = text_parse(open('/Users/elinabian 1/Desktop/CU-life/summerlearning/mlinaction/dataset/email/ham/%d.txt' % i, encoding = \"ISO-8859-1\").read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)           #class 0 represents not spam\n",
    "    vocab_list = create_vocab_list(doc_list)       #create vocabulary list\n",
    "    \n",
    "    \n",
    "    training_set = list(range(50))\n",
    "    test_set=[]                                    #create test set\n",
    "    for i in range(10):                #randomly select 10 of 50 files as training set.\n",
    "        rand_index = int(random.uniform(0,len(training_set)))     #0-50均匀取一个数，一共只取10个数\n",
    "        \n",
    "        # As a number selected, add it to test set and removed from the training set. \n",
    "        # 所以training set 和 test set 是两个index的list\n",
    "        test_set.append(training_set[rand_index])\n",
    "        del(training_set[rand_index]) \n",
    "        \n",
    "        \n",
    "    train_mat=[]\n",
    "    train_classes = []\n",
    "    for doc_index in training_set:      #train the classifier (get probs) train_nb0\n",
    "        train_mat.append(wordbag_to_vec(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    p0_vec, p1_vec, p_spam = train_nb0(np.array(train_mat), np.array(train_classes))\n",
    "    \n",
    "    \n",
    "    error_count = 0\n",
    "    for doc_index in test_set:          #classify the remaining items in test set\n",
    "        word_vector = wordbag_to_vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_nb(np.array(word_vector), p0_vec, p1_vec, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "            print(\"classification error\", doc_list[doc_index])\n",
    "    print('the error rate is: ', float(error_count)/len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "spam_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里10组里错了1组，所以classification error只输出了一个list，但如果有两组错了，这里会输出两个list。\n",
    "\n",
    "可以将以上封装好的程序多运行几次，对错误率求平均值，即可得到一个可信度更高的error rate。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: using naïve Bayes to reveal local attitudes from personal ads\n",
    "We’re going to see if people in different cities use different words\n",
    "### 4.1 Collect: importing RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craiglist.org/sss/index.rss')\n",
    "sf = feedparser.parse('https://sfbay.craigslist.org/sss/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up for sale is a barely used Apple TV with brand new HDMI cables, Power Cable, and OEM Remote. \n",
      "I am moving and do not need it. This was used for a few months max used for less than an 2 hours a week. Adult used, pet free, and smoke free home. First  ...\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(ny['entries'][0]['summary'])\n",
    "print(len(ny['entries']))\n",
    "print(len(sf['entries']))\n",
    "\n",
    "# 不知道为什么只能提取出25个= =、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RSS feed classifier and frequent word removal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the frequency of word occurence\n",
    "\n",
    "def cal_most_freq(vocab_list, full_text):\n",
    "    import operator\n",
    "    freq_dict = {}\n",
    "    for token in vocab_list:\n",
    "        freq_dict[token]=full_text.count(token)\n",
    "    sorted_freq = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)      #dict中排序的方法，KNN也用过\n",
    "    return sorted_freq[:30]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to spam test\n",
    "\n",
    "def local_words(feed1, feed0):\n",
    "    import feedparser\n",
    "    doc_list=[]\n",
    "    class_list = []\n",
    "    full_text =[]\n",
    "    min_len = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(min_len):\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)                             #NY is class 1\n",
    "        word_list = text_parse(feed0['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "        \n",
    "        \n",
    "    vocab_list = create_vocab_list(doc_list)             #create vocabulary\n",
    "    top30_words = cal_most_freq(vocab_list, full_text)   #remove top 30 words\n",
    "    for pair_w in top30_words:\n",
    "        if pair_w[0] in vocab_list: \n",
    "            vocab_list.remove(pair_w[0])                 #list中remove某个元素的方法\n",
    "    \n",
    "    \n",
    "    training_set = list(range(2*min_len))\n",
    "    test_set=[]                                           #create test set\n",
    "    for i in range(5):\n",
    "        rand_index = int(random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_index])\n",
    "        del(training_set[rand_index])  \n",
    "        \n",
    "        \n",
    "    train_mat=[]\n",
    "    train_classes = []\n",
    "    for doc_index in training_set:                        #train the classifier (get probs) trainNB0\n",
    "        train_mat.append(wordbag_to_vec(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    p0_vec, p1_vec, p_spam = train_nb0(np.array(train_mat), np.array(train_classes))\n",
    "    \n",
    "    \n",
    "    error_count = 0\n",
    "    for doc_index in test_set:                            #classify the remaining items\n",
    "        word_vector = wordbag_to_vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_nb(np.array(word_vector), p0_vec, p1_vec, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print('the error rate is: ', float(error_count)/len(test_set))\n",
    "    return vocab_list, p0_vec, p1_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['mountainbike',\n",
       "  'free',\n",
       "  'contact',\n",
       "  'couple',\n",
       "  '650',\n",
       "  'tested',\n",
       "  'shown',\n",
       "  'top',\n",
       "  'back',\n",
       "  'long',\n",
       "  'utf8',\n",
       "  'jailbroken',\n",
       "  'deliver',\n",
       "  'including',\n",
       "  'smoke',\n",
       "  'alexa',\n",
       "  'misc',\n",
       "  'mustang',\n",
       "  'mobile',\n",
       "  'checking',\n",
       "  'undercover',\n",
       "  'vehicle',\n",
       "  'dual',\n",
       "  'children',\n",
       "  'radiator',\n",
       "  'ones',\n",
       "  'yours',\n",
       "  'cannot',\n",
       "  'mattress',\n",
       "  'lightbulbs',\n",
       "  'wii',\n",
       "  'job',\n",
       "  'emblem',\n",
       "  'closet',\n",
       "  'barely',\n",
       "  'left',\n",
       "  'super',\n",
       "  'cover',\n",
       "  'girl',\n",
       "  'audio',\n",
       "  'infinite',\n",
       "  'front',\n",
       "  'od_aui_detailpages02',\n",
       "  'store',\n",
       "  'wieght',\n",
       "  '918',\n",
       "  'pipes',\n",
       "  'delete',\n",
       "  'phones',\n",
       "  'charger',\n",
       "  'urn',\n",
       "  'lego',\n",
       "  'french',\n",
       "  'world',\n",
       "  'firm',\n",
       "  'shape',\n",
       "  '1527089727',\n",
       "  'rb14b',\n",
       "  'cheapest',\n",
       "  'little',\n",
       "  'wood',\n",
       "  'features',\n",
       "  'mid',\n",
       "  'business',\n",
       "  'antique',\n",
       "  'convertible',\n",
       "  'tires',\n",
       "  'qid',\n",
       "  'billing',\n",
       "  'plates',\n",
       "  'machine',\n",
       "  'stain',\n",
       "  'ride',\n",
       "  'already',\n",
       "  'oem',\n",
       "  'collection',\n",
       "  'black',\n",
       "  'press',\n",
       "  'first',\n",
       "  'pin',\n",
       "  'weights',\n",
       "  'lighted',\n",
       "  'watts',\n",
       "  'hdmi',\n",
       "  'playhouse',\n",
       "  'ties',\n",
       "  'sentra',\n",
       "  'victorian',\n",
       "  'few',\n",
       "  'hub',\n",
       "  'customers',\n",
       "  'room',\n",
       "  'grip',\n",
       "  'wheels',\n",
       "  'sign',\n",
       "  'design',\n",
       "  'week',\n",
       "  'less',\n",
       "  'got',\n",
       "  'nomad',\n",
       "  'cable',\n",
       "  'stands',\n",
       "  'hardware',\n",
       "  'freestyle',\n",
       "  'specialized',\n",
       "  'measure',\n",
       "  'below',\n",
       "  'small',\n",
       "  'cents',\n",
       "  'pool',\n",
       "  'amp',\n",
       "  'had',\n",
       "  'v30',\n",
       "  'channel',\n",
       "  'couch',\n",
       "  'boys',\n",
       "  'home',\n",
       "  'kodi',\n",
       "  'modern',\n",
       "  'like',\n",
       "  'etc',\n",
       "  'low',\n",
       "  'xenogear',\n",
       "  'set',\n",
       "  'mechanicall',\n",
       "  'nicely',\n",
       "  'take',\n",
       "  'clearview',\n",
       "  'size',\n",
       "  'large',\n",
       "  'stored',\n",
       "  'its',\n",
       "  '10yr',\n",
       "  'sets',\n",
       "  'how',\n",
       "  'besides',\n",
       "  'description',\n",
       "  'disconnect',\n",
       "  'corner',\n",
       "  '150k',\n",
       "  'ford',\n",
       "  'measures',\n",
       "  'questions',\n",
       "  'chronicles',\n",
       "  'entire',\n",
       "  'block',\n",
       "  'iplay',\n",
       "  'kind',\n",
       "  'further',\n",
       "  'quickstart',\n",
       "  'install',\n",
       "  'aurora',\n",
       "  'styling',\n",
       "  'work',\n",
       "  'gentleman',\n",
       "  'training',\n",
       "  'bayonetta',\n",
       "  'real',\n",
       "  'enjoy',\n",
       "  'lights',\n",
       "  'gold',\n",
       "  'lifecycles',\n",
       "  '4th',\n",
       "  'accident',\n",
       "  'boy',\n",
       "  'door',\n",
       "  'there',\n",
       "  'ipod',\n",
       "  'carvings',\n",
       "  'boost',\n",
       "  'adult',\n",
       "  'warm',\n",
       "  '400',\n",
       "  'road',\n",
       "  'unfraternally',\n",
       "  'often',\n",
       "  'hood',\n",
       "  'pet',\n",
       "  'tabletop',\n",
       "  'inside',\n",
       "  'stick',\n",
       "  'decor',\n",
       "  '300',\n",
       "  'chopper',\n",
       "  'inflates',\n",
       "  'away',\n",
       "  'perfectly',\n",
       "  'project',\n",
       "  'kids',\n",
       "  'conveniently',\n",
       "  'island',\n",
       "  'parkway',\n",
       "  'hid_led_ny',\n",
       "  'wonderful',\n",
       "  'outlet',\n",
       "  'product',\n",
       "  'extended',\n",
       "  'slip',\n",
       "  'stylish',\n",
       "  'instead',\n",
       "  'connection',\n",
       "  'note',\n",
       "  'without',\n",
       "  'day',\n",
       "  'b00hxgsbxc',\n",
       "  'coupe',\n",
       "  'decoration',\n",
       "  'painted',\n",
       "  'arrange',\n",
       "  'automatically',\n",
       "  'single',\n",
       "  'number',\n",
       "  'white',\n",
       "  'die',\n",
       "  'dont',\n",
       "  'ring',\n",
       "  'beam',\n",
       "  'availability',\n",
       "  'demo',\n",
       "  'box',\n",
       "  'iphone',\n",
       "  'bmx',\n",
       "  'tire',\n",
       "  'under',\n",
       "  'track',\n",
       "  'floodlight',\n",
       "  'max',\n",
       "  'getting',\n",
       "  'ballast',\n",
       "  '100',\n",
       "  'old',\n",
       "  'ref',\n",
       "  'marvel',\n",
       "  'hello',\n",
       "  'decades',\n",
       "  'ion',\n",
       "  'washer',\n",
       "  'removed',\n",
       "  'fabric',\n",
       "  'innovative',\n",
       "  'trailer',\n",
       "  'edge',\n",
       "  'reference',\n",
       "  'entry',\n",
       "  'available',\n",
       "  'recessed',\n",
       "  'longer',\n",
       "  'crusty',\n",
       "  'everything',\n",
       "  'lamp',\n",
       "  'now',\n",
       "  'since',\n",
       "  'than',\n",
       "  'whether',\n",
       "  'stand',\n",
       "  'moisture',\n",
       "  'glass',\n",
       "  'trailers',\n",
       "  'mostly',\n",
       "  'represents',\n",
       "  'movie',\n",
       "  'skittle',\n",
       "  'link',\n",
       "  'olympic',\n",
       "  'built',\n",
       "  'sam',\n",
       "  'price',\n",
       "  'please',\n",
       "  'priced',\n",
       "  'plug',\n",
       "  'minor',\n",
       "  'three',\n",
       "  'independent',\n",
       "  'recumbent',\n",
       "  'end',\n",
       "  'original',\n",
       "  'unlock',\n",
       "  'reviews',\n",
       "  'bikes',\n",
       "  'tesla',\n",
       "  'microfiber',\n",
       "  'posting',\n",
       "  'schedule',\n",
       "  'accessories',\n",
       "  'looks',\n",
       "  'about',\n",
       "  'karma',\n",
       "  'game',\n",
       "  'homemade',\n",
       "  'months',\n",
       "  'our',\n",
       "  'andalusian',\n",
       "  'keep',\n",
       "  'enabling',\n",
       "  'activation',\n",
       "  'per',\n",
       "  'that',\n",
       "  'fit',\n",
       "  'were',\n",
       "  'rim',\n",
       "  'support',\n",
       "  'unlike',\n",
       "  'various',\n",
       "  '5ft',\n",
       "  'bell',\n",
       "  'mirror',\n",
       "  'offer',\n",
       "  'probably',\n",
       "  'reply',\n",
       "  'rep',\n",
       "  'personal',\n",
       "  'bmw',\n",
       "  'life',\n",
       "  'over',\n",
       "  'hotrock',\n",
       "  'kits',\n",
       "  'gen',\n",
       "  'interested',\n",
       "  'more',\n",
       "  'send',\n",
       "  'table',\n",
       "  '4xel84',\n",
       "  'belt',\n",
       "  'dream',\n",
       "  'has',\n",
       "  'messages',\n",
       "  'book',\n",
       "  'but',\n",
       "  'att',\n",
       "  'kept',\n",
       "  'jack',\n",
       "  'decent',\n",
       "  'hesitate',\n",
       "  'properly',\n",
       "  'puppy',\n",
       "  'didn',\n",
       "  'sprint',\n",
       "  'chevy',\n",
       "  'digital',\n",
       "  'tables',\n",
       "  'liqour',\n",
       "  'overall',\n",
       "  'highlight',\n",
       "  'wonderfully',\n",
       "  'maker',\n",
       "  'around',\n",
       "  'blocked',\n",
       "  'working',\n",
       "  'fabulous',\n",
       "  'other',\n",
       "  'plastic',\n",
       "  '2051',\n",
       "  'music',\n",
       "  'wall',\n",
       "  'deluxe',\n",
       "  'dried',\n",
       "  'lightning',\n",
       "  'assortment',\n",
       "  'lte',\n",
       "  'come',\n",
       "  'modes',\n",
       "  'wear',\n",
       "  'bsa',\n",
       "  'equipment',\n",
       "  'pound',\n",
       "  'patriotic',\n",
       "  'bumpers',\n",
       "  '990',\n",
       "  'included',\n",
       "  'handful',\n",
       "  'toddler',\n",
       "  'text',\n",
       "  'measuring',\n",
       "  'side',\n",
       "  'lease',\n",
       "  'age',\n",
       "  'complement',\n",
       "  'bought',\n",
       "  'remote',\n",
       "  'uncle',\n",
       "  'near',\n",
       "  'same',\n",
       "  'know',\n",
       "  'sheeran',\n",
       "  'exactly',\n",
       "  'elevated',\n",
       "  'dedicated',\n",
       "  'swim',\n",
       "  'cash',\n",
       "  'sell',\n",
       "  'computer',\n",
       "  'them',\n",
       "  'then',\n",
       "  'preferred',\n",
       "  'unit',\n",
       "  'tree',\n",
       "  'bench',\n",
       "  'line',\n",
       "  'galaxy',\n",
       "  'system',\n",
       "  'apple',\n",
       "  'sitting',\n",
       "  'header',\n",
       "  'scene',\n",
       "  'stickers',\n",
       "  'piece',\n",
       "  'mechanical',\n",
       "  'com',\n",
       "  'rotors',\n",
       "  'does',\n",
       "  'could',\n",
       "  'seen',\n",
       "  'nice',\n",
       "  'six',\n",
       "  'verizon',\n",
       "  'watt',\n",
       "  'ceiling',\n",
       "  'storage',\n",
       "  '500cc',\n",
       "  'bed',\n",
       "  'gps',\n",
       "  'petaluma',\n",
       "  'runs',\n",
       "  'matching',\n",
       "  'tail',\n",
       "  'heater',\n",
       "  'amazing',\n",
       "  'early',\n",
       "  'meet',\n",
       "  'grade',\n",
       "  'info',\n",
       "  'bar',\n",
       "  'gross',\n",
       "  'keys',\n",
       "  'warning',\n",
       "  'circuit',\n",
       "  'doors',\n",
       "  'quick',\n",
       "  'speed',\n",
       "  'power',\n",
       "  'shade',\n",
       "  'ooma',\n",
       "  'inquiries',\n",
       "  'distinguishes',\n",
       "  'information',\n",
       "  'series',\n",
       "  'seats',\n",
       "  'frame',\n",
       "  'buying',\n",
       "  'water',\n",
       "  'sealed',\n",
       "  'sale',\n",
       "  'tickets',\n",
       "  'works',\n",
       "  'www',\n",
       "  'going',\n",
       "  'shows',\n",
       "  'hours',\n",
       "  'pic',\n",
       "  'indoors',\n",
       "  'time',\n",
       "  'make',\n",
       "  '48ft',\n",
       "  'office',\n",
       "  'anymore',\n",
       "  'each',\n",
       "  'decorated',\n",
       "  'cant',\n",
       "  'looking',\n",
       "  'firework',\n",
       "  'listen',\n",
       "  'voice',\n",
       "  'approximately',\n",
       "  'their',\n",
       "  'fastback',\n",
       "  'kart',\n",
       "  'honda',\n",
       "  'running',\n",
       "  'using',\n",
       "  'lock',\n",
       "  'comes',\n",
       "  'several',\n",
       "  '676k',\n",
       "  'commercial',\n",
       "  'texture',\n",
       "  'was',\n",
       "  'years',\n",
       "  'record',\n",
       "  'electrical',\n",
       "  'floral',\n",
       "  'covers',\n",
       "  'leaks',\n",
       "  'wolf',\n",
       "  'think',\n",
       "  'details',\n",
       "  'turntable',\n",
       "  'riding',\n",
       "  'cleaning',\n",
       "  '650cc',\n",
       "  'crack',\n",
       "  'model',\n",
       "  'email',\n",
       "  'when',\n",
       "  'right',\n",
       "  'peachpit',\n",
       "  'park',\n",
       "  'expressway',\n",
       "  'two',\n",
       "  'inches',\n",
       "  'these',\n",
       "  '184',\n",
       "  'will',\n",
       "  'told',\n",
       "  'includes',\n",
       "  'needs',\n",
       "  'shoes',\n",
       "  'pick',\n",
       "  'too',\n",
       "  'problems',\n",
       "  'still',\n",
       "  'most',\n",
       "  'monopoly',\n",
       "  'decorative',\n",
       "  'watch',\n",
       "  'collars',\n",
       "  'visual',\n",
       "  'fogging',\n",
       "  'handsome',\n",
       "  'buy',\n",
       "  'switch',\n",
       "  '4cilinder',\n",
       "  'fan',\n",
       "  'well',\n",
       "  'very',\n",
       "  'bathroom',\n",
       "  '390',\n",
       "  'dumbbell',\n",
       "  'metal',\n",
       "  'fire',\n",
       "  'leather',\n",
       "  'comic',\n",
       "  'want',\n",
       "  'inch',\n",
       "  'b01f4o0ta2',\n",
       "  'miles',\n",
       "  'transfer',\n",
       "  'trou',\n",
       "  'buckskin',\n",
       "  'california',\n",
       "  'cloth',\n",
       "  'one',\n",
       "  'via',\n",
       "  'player',\n",
       "  'venezuela',\n",
       "  'mountain',\n",
       "  'pix',\n",
       "  'garage',\n",
       "  'throw',\n",
       "  'pixel',\n",
       "  '101',\n",
       "  'accord',\n",
       "  'light',\n",
       "  'tall',\n",
       "  'month',\n",
       "  '5x12ax7',\n",
       "  'nissan',\n",
       "  'amazon',\n",
       "  '1980',\n",
       "  'many',\n",
       "  'https',\n",
       "  'wth',\n",
       "  'channels',\n",
       "  'pics',\n",
       "  'fenders',\n",
       "  'thinking',\n",
       "  'gopro',\n",
       "  'only',\n",
       "  'cheaper',\n",
       "  'horse',\n",
       "  'alarm',\n",
       "  'drilled',\n",
       "  'gas',\n",
       "  '2003',\n",
       "  'maintained',\n",
       "  'components',\n",
       "  'dyna',\n",
       "  'listing',\n",
       "  'get',\n",
       "  'don',\n",
       "  'provide',\n",
       "  'cross',\n",
       "  'code',\n",
       "  '430',\n",
       "  'videos',\n",
       "  'beige',\n",
       "  'fee',\n",
       "  'true',\n",
       "  'discs',\n",
       "  'parts',\n",
       "  'porcelain',\n",
       "  'configuration',\n",
       "  'hbo',\n",
       "  'mp3',\n",
       "  'need',\n",
       "  'being',\n",
       "  'city',\n",
       "  'guides',\n",
       "  'google',\n",
       "  '125',\n",
       "  'use',\n",
       "  'manuals',\n",
       "  'suede',\n",
       "  'anywa',\n",
       "  'rusty',\n",
       "  'blast',\n",
       "  'moving',\n",
       "  'look',\n",
       "  'huy',\n",
       "  '1968',\n",
       "  'non',\n",
       "  'slotted',\n",
       "  'anyone',\n",
       "  'off',\n",
       "  'weeks',\n",
       "  'samsung',\n",
       "  'comfy',\n",
       "  'showtime',\n",
       "  'beautiful',\n",
       "  'believe',\n",
       "  'civic',\n",
       "  'fully',\n",
       "  'almost',\n",
       "  'cold',\n",
       "  'tonight',\n",
       "  'duty',\n",
       "  'sales',\n",
       "  'mario',\n",
       "  'stallion',\n",
       "  'ultra',\n",
       "  'might',\n",
       "  'cables',\n",
       "  'romantic',\n",
       "  'key',\n",
       "  'remember',\n",
       "  'pushed',\n",
       "  'records',\n",
       "  'handcrafted',\n",
       "  'any',\n",
       "  'after',\n",
       "  'receiver',\n",
       "  'heating',\n",
       "  'resistance',\n",
       "  'july',\n",
       "  'damp',\n",
       "  'monthly',\n",
       "  'sr_1_1',\n",
       "  'asking',\n",
       "  'played',\n",
       "  'great',\n",
       "  'been',\n",
       "  'what',\n",
       "  'chrome',\n",
       "  'title',\n",
       "  'strong',\n",
       "  'automotive',\n",
       "  'came',\n",
       "  'exercise',\n",
       "  'never',\n",
       "  'comp',\n",
       "  'proven',\n",
       "  'located',\n",
       "  'rocket',\n",
       "  'also',\n",
       "  'rear',\n",
       "  'comics',\n",
       "  'airblow',\n",
       "  'window',\n",
       "  'excellent',\n",
       "  'pitted',\n",
       "  'must',\n",
       "  'rent',\n",
       "  'check',\n",
       "  'here',\n",
       "  'brown',\n",
       "  'living',\n",
       "  'ask',\n",
       "  'vague',\n",
       "  'ebay',\n",
       "  'christmas',\n",
       "  'needed',\n",
       "  'damage',\n",
       "  'pieces',\n",
       "  'ps4',\n",
       "  'usb',\n",
       "  'games',\n",
       "  'gain',\n",
       "  'way',\n",
       "  'celebration',\n",
       "  'just',\n",
       "  'royalbaby',\n",
       "  'let',\n",
       "  'car',\n",
       "  'case',\n",
       "  'section',\n",
       "  'xtra',\n",
       "  'times',\n",
       "  'mounted',\n",
       "  'phone',\n",
       "  'obo',\n",
       "  'gorgeous'],\n",
       " [-5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -4.814214812922799,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -4.591071261608589,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -4.591071261608589,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -4.814214812922799,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -4.814214812922799,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -4.591071261608589,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -4.814214812922799,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -4.814214812922799,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -4.814214812922799,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -4.814214812922799,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -4.814214812922799,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -5.101896885374581,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -5.5073619934827445,\n",
       "  -6.20050917404269,\n",
       "  -6.20050917404269,\n",
       "  -5.101896885374581,\n",
       "  -5.5073619934827445],\n",
       " [-6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.426043520090656,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.243721963296701,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.6491870714048655,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -4.6491870714048655,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -4.6491870714048655,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -4.936869143856646,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -4.936869143856646,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -5.342334251964811,\n",
       "  -6.035481432524756])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_words(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can comment out the three lines that removed the most frequently used words and see the performance before and after. Error rate is 54% without these lines and 70% with the lines included. \n",
    "\n",
    "An interesting observation is that the top 30 words in these posts make up close to 30% of all the words used. The size of the *vocab_list* was about 3000 words when I was testing this. A small percentage of the total words makes up a large portion of the text. \n",
    "\n",
    "The reason for this is that a large percentage of language is *redundancy and structural glue*. \n",
    "\n",
    "Another common approach is to not just remove the most common words but to also remove this structural glue from a predefined list. This is known as a *stop word list*, and there are a number of sources of this available. \n",
    "<br><br>\n",
    "<font color = 'green'>这段话想证明的问题，仅仅是top words占了很大一部分。至于top words对于分类问题的影响，感觉应该是需要被忽略的，然鹅这里的结果却是保留这些词的时候，分类错误率更低= =、</font>\n",
    "\n",
    "### 4.3 Analyze: displaying locally used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most descriptive word display function\n",
    "\n",
    "def get_top_words(ny, sf):\n",
    "    import operator\n",
    "    vocab_list, p0_vec, p1_vec = local_words(ny, sf)\n",
    "    top_ny=[]\n",
    "    top_sf=[]\n",
    "    for i in range(len(p0_vec)):\n",
    "        if p0_vec[i] > -5.0 :          #之前取了log，所以是negative\n",
    "                                       #这里没取top words，而是取了超过某个阈值的所有words\n",
    "            top_sf.append((vocab_list[i], p0_vec[i]))\n",
    "        if p1_vec[i] > -5.0 : \n",
    "            top_ny.append((vocab_list[i], p1_vec[i]))\n",
    "            \n",
    "            \n",
    "    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sorted_sf:\n",
    "        print(item[0])\n",
    "        \n",
    "        \n",
    "    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sorted_ny:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.2\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "weights\n",
      "lamp\n",
      "bench\n",
      "each\n",
      "wieght\n",
      "bar\n",
      "seats\n",
      "right\n",
      "buy\n",
      "get\n",
      "games\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "delete\n",
      "game\n",
      "that\n",
      "sale\n",
      "few\n",
      "channel\n",
      "block\n",
      "old\n",
      "everything\n",
      "has\n",
      "book\n",
      "liqour\n",
      "power\n",
      "inquiries\n",
      "approximately\n",
      "comes\n",
      "well\n",
      "very\n",
      "great\n",
      "never\n",
      "vague\n"
     ]
    }
   ],
   "source": [
    "get_top_words(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words from this output are entertaining. \n",
    "\n",
    "One thing to note: a lot of stop words appear in the output. It would be interesting to see how things would change if you removed the *fixed stop words*. In my experience, the classification error will also go down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
